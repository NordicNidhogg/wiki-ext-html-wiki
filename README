This extension to MediaWiki is used to import HTML content into the wiki.

It requires the Parsoid extension, and relies on that to transform an HTML DOM
into wikitext.  More at http://www.mediawiki.org/wiki/Parsoid

User nominates upload of file via upload form similar to upload image
User also specifies the "collection" name of where this source is coming from 
(e.g. The URL or website)

How does upload work?
Parsoid.hooks.php has a hook onFileUpload, which refers to filerepo/file/LocalFile.php
If I'm reading it correctly, LocalFile.php is mostly called from a Repo object.

LocalFile::upload() and recordUpload() are informative.

Later, this will need to handle zip uploads.

With zip uploads, we will need to pre-process the content to identify the
essential content, and remove navigation, JavaScript, presentational graphics, etc.

Form file content is saved to server (tmp), and that triggers conversion attempt.
A Title is proposed from text (checked in the db), and user can override naming
HTML is converted to wiki text for the content of the article.

Image references are extracted from source

For each of those images, the image asset is retrieved, uploaded with automatic 
file naming based on the "prefix" + src attribute.

Also, each image is tagged with the collection name for easier identification

Once all the images are contained in the wiki, the wiki markup for the article 
can be updated to reference those images.  IOW, it may be possible to upload
an HTML source file, and batch a job to import all images AND update the article
source to use the images... Or, since we know ahead of time what the image file 
name will be, we can just reference the non-existant images in the article.  They
will exist in the wiki after a short delay required to fetch and process the 
image files.

(Eliminate duplicate images based on checksum?)

What, if any, additional tables do we need in the database? 
https://www.mediawiki.org/wiki/Manual:Hooks/LoadExtensionSchemaUpdates
We may need to store checksums for zip uploads, because we don't want to store
the zip itself, but we may want to recognize a re-upload attempt?

Logging is provided at [[Special:Log/html2wiki]]  The facility for logging will
tap into LogEntry as outlined at
https://www.mediawiki.org/wiki/Manual:Logging_to_Special:Log
Interestingly, SpecialUpload must call LogEntry from it's hooks  SpecialImport
calls LogPage which itself invokes LogEntry (see includes/logging).

More information can be found at https://www.mediawiki.org/wiki/Extension:Html2Wiki
Current development happens at ???????
* @todo publish the extension upstream. first with the @link http://www.mediawiki.org/wiki/Template:Extension
git clone https://gerrit.wikimedia.org/r/p/mediawiki/
or (with gerrit auth)
git clone ssh://USERNAME@gerrit.wikimedia.org:29418/mediawiki/services/parsoid